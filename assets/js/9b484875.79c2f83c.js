"use strict";(self.webpackChunklrvinye_blog=self.webpackChunklrvinye_blog||[]).push([[615],{3905:function(e,n,t){t.d(n,{Zo:function(){return u},kt:function(){return p}});var a=t(7294);function r(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function s(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function o(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?s(Object(t),!0).forEach((function(n){r(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):s(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function i(e,n){if(null==e)return{};var t,a,r=function(e,n){if(null==e)return{};var t,a,r={},s=Object.keys(e);for(a=0;a<s.length;a++)t=s[a],n.indexOf(t)>=0||(r[t]=e[t]);return r}(e,n);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(a=0;a<s.length;a++)t=s[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var l=a.createContext({}),c=function(e){var n=a.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):o(o({},n),e)),t},u=function(e){var n=c(e.components);return a.createElement(l.Provider,{value:n},e.children)},m={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},d=a.forwardRef((function(e,n){var t=e.components,r=e.mdxType,s=e.originalType,l=e.parentName,u=i(e,["components","mdxType","originalType","parentName"]),d=c(t),p=r,b=d["".concat(l,".").concat(p)]||d[p]||m[p]||s;return t?a.createElement(b,o(o({ref:n},u),{},{components:t})):a.createElement(b,o({ref:n},u))}));function p(e,n){var t=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var s=t.length,o=new Array(s);o[0]=d;var i={};for(var l in n)hasOwnProperty.call(n,l)&&(i[l]=n[l]);i.originalType=e,i.mdxType="string"==typeof e?e:r,o[1]=i;for(var c=2;c<s;c++)o[c]=t[c];return a.createElement.apply(null,o)}return a.createElement.apply(null,t)}d.displayName="MDXCreateElement"},857:function(e,n,t){t.r(n),t.d(n,{frontMatter:function(){return i},contentTitle:function(){return l},metadata:function(){return c},toc:function(){return u},default:function(){return d}});var a=t(7462),r=t(3366),s=(t(7294),t(3905)),o=["components"],i={title:"\u901a\u8fc7 Helm \u90e8\u7f72 RabbitMQ \u96c6\u7fa4",date:new Date("2021-04-01T21:41:45.000Z"),tags:["Helm","K8S","RabbitMQ","Bitnami","\u5b89\u88c5","\u914d\u7f6e"],category:"\u8fd0\u7ef4"},l=void 0,c={unversionedId:"devops/deployRabbitMqClusterByHelm",id:"devops/deployRabbitMqClusterByHelm",isDocsHomePage:!1,title:"\u901a\u8fc7 Helm \u90e8\u7f72 RabbitMQ \u96c6\u7fa4",description:"\u4e0b\u9762\u5c06\u4ecb\u7ecd\uff0c\u5728 K8S \u96c6\u7fa4\u4e2d\u901a\u8fc7 Bitnami \u7684 Helm Chart \u642d\u5efa\u90e8\u7f72\u4e00\u4e2a\u8d1f\u8f7d\u5747\u8861\u67b6\u6784 3 \u8282\u70b9\u7684 RabbitMQ \u96c6\u7fa4",source:"@site/docs/devops/deployRabbitMqClusterByHelm.md",sourceDirName:"devops",slug:"/devops/deployRabbitMqClusterByHelm",permalink:"/blog/docs/devops/deployRabbitMqClusterByHelm",editUrl:"https://github.com/facebook/docusaurus/edit/master/website/docs/devops/deployRabbitMqClusterByHelm.md",version:"current",frontMatter:{title:"\u901a\u8fc7 Helm \u90e8\u7f72 RabbitMQ \u96c6\u7fa4",date:"2021-04-01T21:41:45.000Z",tags:["Helm","K8S","RabbitMQ","Bitnami","\u5b89\u88c5","\u914d\u7f6e"],category:"\u8fd0\u7ef4"},sidebar:"devops",previous:{title:"\u901a\u8fc7 Helm \u90e8\u7f72 Postgres \u96c6\u7fa4",permalink:"/blog/docs/devops/deployPostgresClusterByHelm"},next:{title:"\u901a\u8fc7 Helm \u90e8\u7f72 Redis \u96c6\u7fa4",permalink:"/blog/docs/devops/deployRedisClusterByHelm"}},u=[{value:"\u51c6\u5907\u6301\u4e45\u5316",id:"\u51c6\u5907\u6301\u4e45\u5316",children:[]},{value:"\u51c6\u5907 Values.yaml",id:"\u51c6\u5907-valuesyaml",children:[{value:"<strong>\u5168\u5c40\u53c2\u6570</strong>",id:"\u5168\u5c40\u53c2\u6570",children:[]},{value:"<strong>\u8282\u70b9\u8bbe\u7f6e</strong>",id:"\u8282\u70b9\u8bbe\u7f6e",children:[]},{value:"<strong>\u6301\u4e45\u5316\u914d\u7f6e</strong>",id:"\u6301\u4e45\u5316\u914d\u7f6e",children:[]},{value:"\u521d\u59cb\u5316\u65f6\u76ee\u5f55\u6743\u9650\u914d\u7f6e",id:"\u521d\u59cb\u5316\u65f6\u76ee\u5f55\u6743\u9650\u914d\u7f6e",children:[]}]}],m={toc:u};function d(e){var n=e.components,t=(0,r.Z)(e,o);return(0,s.kt)("wrapper",(0,a.Z)({},m,t,{components:n,mdxType:"MDXLayout"}),(0,s.kt)("p",null,"\u4e0b\u9762\u5c06\u4ecb\u7ecd\uff0c\u5728 K8S \u96c6\u7fa4\u4e2d\u901a\u8fc7 Bitnami \u7684 Helm Chart \u642d\u5efa\u90e8\u7f72\u4e00\u4e2a\u8d1f\u8f7d\u5747\u8861\u67b6\u6784 3 \u8282\u70b9\u7684 RabbitMQ \u96c6\u7fa4"),(0,s.kt)("h1",{id:"\u73af\u5883"},"\u73af\u5883"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"\u817e\u8baf\u4e91 kubernetes 1.18.4-tke.6"),(0,s.kt)("li",{parentName:"ul"},"Bitnami RabbitMQ-8.11.5 ",(0,s.kt)("a",{parentName:"li",href:"https://charts.bitnami.com/bitnami/rabbitmq-8.11.5.tgz"},"Chart \u4e0b\u8f7d"))),(0,s.kt)("h1",{id:"\u5177\u4f53\u6b65\u9aa4"},"\u5177\u4f53\u6b65\u9aa4"),(0,s.kt)("h2",{id:"\u51c6\u5907\u6301\u4e45\u5316"},"\u51c6\u5907\u6301\u4e45\u5316"),(0,s.kt)("p",null,"\u8fd9\u91cc\u4f7f\u7528\u817e\u8baf\u4e91\u7684\u4e91\u786c\u76d8\u4f5c\u4e3a\u6301\u4e45\u5316\u5377"),(0,s.kt)("ol",null,(0,s.kt)("li",{parentName:"ol"},"\u624b\u52a8\u521b\u5efa \u6240\u9700\u7684\u4e91\u786c\u76d8\uff0c\u8fd9\u91cc\u6211\u4eec\u6709 3 \u4e2a\u8282\u70b9\uff0c\u9700\u8981 3 \u4e2a\u4e91\u786c\u76d8\uff0c\u6bcf\u4e2a 10GB"),(0,s.kt)("li",{parentName:"ol"},"\u521b\u5efa\u4e0e\u4e91\u786c\u76d8\u5bf9\u5e94\u7684 ",(0,s.kt)("inlineCode",{parentName:"li"},"storageClass")," \u7531\u4e8e\u662f\u6570\u636e\u5e93\u6301\u4e45\u5316\u578b\u7684\u6570\u636e\uff0c\u6211\u4eec\u5e94\u8be5\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u6307\u5b9a\u56de\u6536\u7b56\u7565\u4e3a ",(0,s.kt)("inlineCode",{parentName:"li"},"Retain"),"\uff0c\u5377\u7ed1\u5b9a\u6a21\u5f0f\u5e94\u8be5\u8bbe\u7f6e\u4e3a",(0,s.kt)("inlineCode",{parentName:"li"},"\u7acb\u5373\u7ed1\u5b9a")),(0,s.kt)("li",{parentName:"ol"},"\u521b\u5efa\u6307\u5b9a",(0,s.kt)("inlineCode",{parentName:"li"},"storageClass"),"\u7684 PV \u5377 \u5e76\u5728\u63a7\u5236\u53f0\u4e2d\u4e0e\u4e0a\u4e00\u6b65\u521b\u5efa\u7684\u4e91\u786c\u76d8\u64cd\u4f5c\u8fdb\u884c\u7ed1\u5b9a"),(0,s.kt)("li",{parentName:"ol"},"PV \u9700\u8981\u6253\u4e0a labels")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-yaml"},"labels:\n  app: rabbitmq-node\n")),(0,s.kt)("p",null,"\u8fd9\u91cc\u7684 label \u5c06\u5728\u4e0b\u9762\u7684\u6b65\u9aa4\u4e2d\u7528\u6765\u5bf9 PVC \u8fdb\u884c\u5339\u914d"),(0,s.kt)("h2",{id:"\u51c6\u5907-valuesyaml"},"\u51c6\u5907 Values.yaml"),(0,s.kt)("blockquote",null,(0,s.kt)("p",{parentName:"blockquote"},"\u4e0b\u9762\u7684 Values \u914d\u7f6e\u4e2d\uff0c\u4ec5\u6307\u51fa\u9700\u8981\u6839\u636e",(0,s.kt)("a",{parentName:"p",href:"#Values%E5%8E%9F%E6%A8%A1%E6%9D%BF%E9%85%8D%E7%BD%AE"},"\u6a21\u677f"),"\u7684\u539f\u914d\u7f6e\u4fe1\u606f\u8fdb\u884c\u81ea\u5b9a\u4e49\u7684\u9009\u9879\uff01\uff01\uff01")),(0,s.kt)("h3",{id:"\u5168\u5c40\u53c2\u6570"},(0,s.kt)("strong",{parentName:"h3"},"\u5168\u5c40\u53c2\u6570")),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"\u6307\u5b9a ",(0,s.kt)("inlineCode",{parentName:"li"},"storageClass")," \u4e3a\u4e0a\u9762\u6211\u4eec\u5df2\u7ecf\u521b\u5efa\u7684 SC")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-yaml"},"global:\n  storageClass: cbs-db-sc-test\n")),(0,s.kt)("h3",{id:"\u8282\u70b9\u8bbe\u7f6e"},(0,s.kt)("strong",{parentName:"h3"},"\u8282\u70b9\u8bbe\u7f6e")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-yaml"},"auth:\n  ## \u7528\u6237\u540d\n  username: lrvinye\n  ## \u5bc6\u7801 \uff0c\u8fd9\u91cc\u7684\u5bc6\u7801\u4e0d\u662f\u9ed8\u8ba4\u751f\u6548\u7684\uff0c\u9700\u8981\u5728\u4e0b\u9762\u7684\u914d\u7f6e\u6587\u4ef6\u4e2d\u914d\u7f6e\n  password: xy000409\n\n  ## \u8fd9\u91cc\u7684default_pass\u9700\u8981\u624b\u52a8\u66f4\u6539\u6210\u4e0b\u9762\u7684\u5199\u6cd5\uff0c\u5426\u5219\u4e0a\u9762\u914d\u7f6e\u7684\u5bc6\u7801\u65e0\u6cd5\u751f\u6548\nconfiguration: |-\n  {{- if not .Values.loadDefinition.enabled -}}\n  ## Username and password\n  ##\n  default_user = {{ .Values.auth.username }}\n  default_pass = {{ .Values.auth.password }}\n  {{- end }}\n\nreplicaCount: 3\n\nnodeSelector:\n  #\u8fd9\u91cc\u662f\u7528\u6765\u7ea6\u675f\u8282\u70b9\u7684\u6240\u5728node\uff0c\u56e0\u4e3a\u817e\u8baf\u4e91\u7684\u4e91\u786c\u76d8\u53ea\u80fd\u6302\u8f7d\u540c\u4e00\u53ef\u7528\u533a\u4e0b\u7684CVM\n  topology.com.tencent.cloud.csi.cbs/zone: ap-guangzhou-4\n\nresources:\n## \u6839\u636e\u81ea\u5df1\u670d\u52a1\u5668\u7684\u53ef\u7528\u8d44\u6e90\u6765\u81ea\u5b9a\u4e49\uff0c\u8fd9\u91cc\u7684\u503c\u76ee\u524d\u53ef\u7528\u6b63\u5e38\u542f\u52a8\uff0c\u8fc7\u4f4e\u7684\u914d\u7f6e\u5c06\u65e0\u6cd5\u6210\u529f\u542f\u52a8\n  limits: \n    cpu: 350m\n    memory: 1024Mi\n  requests: \n    cpu: 100m\n    memory: 150Mi\n\n")),(0,s.kt)("h3",{id:"\u6301\u4e45\u5316\u914d\u7f6e"},(0,s.kt)("strong",{parentName:"h3"},"\u6301\u4e45\u5316\u914d\u7f6e")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-yaml"},"## \u6301\u4e45\u5316\npersistence:\n  enabled: true\n  selector:\n    matchLabels:\n      app: rbmq-node\n  accessMode: ReadWriteOnce\n  ## If you change this value, you might have to adjust `rabbitmq.diskFreeLimit` as well.\n  ##\n  size: 10Gi\n")),(0,s.kt)("h3",{id:"\u521d\u59cb\u5316\u65f6\u76ee\u5f55\u6743\u9650\u914d\u7f6e"},"\u521d\u59cb\u5316\u65f6\u76ee\u5f55\u6743\u9650\u914d\u7f6e"),(0,s.kt)("p",null,"\u4e3a\u4e86\u9632\u6b62\u6743\u9650\u4e0d\u8db3",(0,s.kt)("inlineCode",{parentName:"p"},"Permission deny")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-yaml"},"volumePermissions:\n  enabled: true\n")),(0,s.kt)("hr",null),(0,s.kt)("p",null,"Chart \u51c6\u5907\u5b8c\u6bd5\u5373\u53ef\u8fdb\u884c\u90e8\u7f72"),(0,s.kt)("h1",{id:"values-\u539f\u6a21\u677f\u914d\u7f6e"},"Values \u539f\u6a21\u677f\u914d\u7f6e"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-yaml"},'## Global Docker image parameters\n## Please, note that this will override the image parameters, including dependencies, configured to use the global value\n## Current available global Docker image parameters: imageRegistry and imagePullSecrets\n##\n# global:\n#   imageRegistry: myRegistryName\n#   imagePullSecrets:\n#     - myRegistryKeySecretName\n#   storageClass: myStorageClass\n\n## Bitnami RabbitMQ image version\n## ref: https://hub.docker.com/r/bitnami/rabbitmq/tags/\n##\nimage:\n  registry: docker.io\n  repository: bitnami/rabbitmq\n  tag: 3.8.14-debian-10-r24\n\n  ## set to true if you would like to see extra information on logs\n  ## It turns BASH and/or NAMI debugging in the image\n  ##\n  debug: false\n\n  ## Specify a imagePullPolicy\n  ## Defaults to \'Always\' if image tag is \'latest\', else set to \'IfNotPresent\'\n  ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images\n  ##\n  pullPolicy: IfNotPresent\n  ## Optionally specify an array of imagePullSecrets.\n  ## Secrets must be manually created in the namespace.\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n  ##\n  # pullSecrets:\n  #   - myRegistryKeySecretName\n\n## String to partially override rabbitmq.fullname template (will maintain the release name)\n##\n# nameOverride:\n\n## String to fully override rabbitmq.fullname template\n##\n# fullnameOverride:\n\n## Force target Kubernetes version (using Helm capabilites if not set)\n##\nkubeVersion:\n\n## Kubernetes Cluster Domain\n##\nclusterDomain: cluster.local\n\n## Deployment pod host aliases\n## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/\n##\nhostAliases: []\n\n## RabbitMQ Authentication parameters\n##\nauth:\n  ## RabbitMQ application username\n  ## ref: https://github.com/bitnami/bitnami-docker-rabbitmq#environment-variables\n  ##\n  username: user\n\n  ## RabbitMQ application password\n  ## ref: https://github.com/bitnami/bitnami-docker-rabbitmq#environment-variables\n  ##\n  # password:\n  # existingPasswordSecret: name-of-existing-secret\n\n  ## Erlang cookie to determine whether different nodes are allowed to communicate with each other\n  ## ref: https://github.com/bitnami/bitnami-docker-rabbitmq#environment-variables\n  ##\n  # erlangCookie:\n  # existingErlangSecret: name-of-existing-secret\n\n  ## Enable encryption to rabbitmq\n  ## ref: https://www.rabbitmq.com/ssl.html\n  ##\n  tls:\n    enabled: false\n    failIfNoPeerCert: true\n    sslOptionsVerify: verify_peer\n    caCertificate: |-\n    serverCertificate: |-\n    serverKey: |-\n    # existingSecret: name-of-existing-secret-to-rabbitmq\n    existingSecretFullChain: false\n\n## Value for the RABBITMQ_LOGS environment variable\n## ref: https://www.rabbitmq.com/logging.html#log-file-location\n##\nlogs: \'-\'\n\n## RabbitMQ Max File Descriptors\n## ref: https://github.com/bitnami/bitnami-docker-rabbitmq#environment-variables\n## ref: https://www.rabbitmq.com/install-debian.html#kernel-resource-limits\n##\nulimitNofiles: \'65536\'\n\n## RabbitMQ maximum available scheduler threads and online scheduler threads. By default it will create a thread per CPU detected, with the following parameters you can tune it manually.\n## ref: https://hamidreza-s.github.io/erlang/scheduling/real-time/preemptive/migration/2016/02/09/erlang-scheduler-details.html#scheduler-threads\n## ref: https://github.com/bitnami/charts/issues/2189\n##\n# maxAvailableSchedulers: 2\n# onlineSchedulers: 1\n\n## The memory threshold under which RabbitMQ will stop reading from client network sockets, in order to avoid being killed by the OS\n## ref: https://www.rabbitmq.com/alarms.html\n## ref: https://www.rabbitmq.com/memory.html#threshold\n##\nmemoryHighWatermark:\n  enabled: false\n  ## Memory high watermark type. Either absolute or relative\n  ##\n  type: \'relative\'\n  ## Memory high watermark value.\n  ## The default value of 0.4 stands for 40% of available RAM\n  ## Note: the memory relative limit is applied to the resource.limits.memory to calculate the memory threshold\n  ## You can also use an absolute value, e.g.: 256MB\n  ##\n  value: 0.4\n\n## Plugins to enable\n##\nplugins: \'rabbitmq_management rabbitmq_peer_discovery_k8s\'\n\n## Community plugins to download during container initialization.\n## Combine it with extraPlugins to also enable them.\n##\n# communityPlugins:\n\n## Extra plugins to enable\n## Use this instead of `plugins` to add new plugins\n##\nextraPlugins: \'rabbitmq_auth_backend_ldap\'\n\n## Clustering settings\n##\nclustering:\n  addressType: hostname\n  ## Rebalance master for queues in cluster when new replica is created\n  ## ref: https://www.rabbitmq.com/rabbitmq-queues.8.html#rebalance\n  ##\n  rebalance: false\n\n  ## forceBoot: executes \'rabbitmqctl force_boot\' to force boot cluster shut down unexpectedly in an\n  ## unknown order.\n  ## ref: https://www.rabbitmq.com/rabbitmqctl.8.html#force_boot\n  ##\n  forceBoot: false\n\n## Loading a RabbitMQ definitions file to configure RabbitMQ\n##\nloadDefinition:\n  enabled: false\n  ## Can be templated if needed, e.g.\n  ## existingSecret: "{{ .Release.Name }}-load-definition"\n  ##\n  # existingSecret:\n\n## Command and args for running the container (set to default if not set). Use array form\n##\n# command:\n# args:\n\n## Default duration in seconds k8s waits for container to exit before sending kill signal. Any time in excess of\n## 10 seconds will be spent waiting for any synchronization necessary for cluster not to lose data.\n##\nterminationGracePeriodSeconds: 120\n\n## Additional environment variables to set\n## E.g:\n## extraEnvVars:\n##   - name: FOO\n##     value: BAR\n##\nextraEnvVars: []\n\n## ConfigMap with extra environment variables\n##\n# extraEnvVarsCM:\n\n## Secret with extra environment variables\n##\n# extraEnvVarsSecret:\n\n## Extra ports to be included in container spec, primarily informational\n## E.g:\n## extraContainerPorts:\n## - name: new_port_name\n##   containerPort: 1234\n##\nextraContainerPorts: []\n\n## Configuration file content: required cluster configuration\n## Do not override unless you know what you are doing.\n## To add more configuration, use `extraConfiguration` of `advancedConfiguration` instead\n##\nconfiguration: |-\n  {{- if not .Values.loadDefinition.enabled -}}\n  ## Username and password\n  ##\n  default_user = {{ .Values.auth.username }}\n  default_pass = CHANGEME\n  {{- end }}\n  ## Clustering\n  ##\n  cluster_formation.peer_discovery_backend  = rabbit_peer_discovery_k8s\n  cluster_formation.k8s.host = kubernetes.default.svc.{{ .Values.clusterDomain }}\n  cluster_formation.node_cleanup.interval = 10\n  cluster_formation.node_cleanup.only_log_warning = true\n  cluster_partition_handling = autoheal\n  # queue master locator\n  queue_master_locator = min-masters\n  # enable guest user\n  loopback_users.guest = false\n  {{ tpl .Values.extraConfiguration . }}\n  {{- if .Values.auth.tls.enabled }}\n  ssl_options.verify = {{ .Values.auth.tls.sslOptionsVerify }}\n  listeners.ssl.default = {{ .Values.service.tlsPort }}\n  ssl_options.fail_if_no_peer_cert = {{ .Values.auth.tls.failIfNoPeerCert }}\n  ssl_options.cacertfile = /opt/bitnami/rabbitmq/certs/ca_certificate.pem\n  ssl_options.certfile = /opt/bitnami/rabbitmq/certs/server_certificate.pem\n  ssl_options.keyfile = /opt/bitnami/rabbitmq/certs/server_key.pem\n  {{- end }}\n  {{- if .Values.ldap.enabled }}\n  auth_backends.1 = rabbit_auth_backend_ldap\n  auth_backends.2 = internal\n  {{- range $index, $server := .Values.ldap.servers }}\n  auth_ldap.servers.{{ add $index 1 }} = {{ $server }}\n  {{- end }}\n  auth_ldap.port = {{ .Values.ldap.port }}\n  auth_ldap.user_dn_pattern = {{ .Values.ldap.user_dn_pattern  }}\n  {{- if .Values.ldap.tls.enabled }}\n  auth_ldap.use_ssl = true\n  {{- end }}\n  {{- end }}\n  {{- if .Values.metrics.enabled }}\n  ## Prometheus metrics\n  ##\n  prometheus.tcp.port = 9419\n  {{- end }}\n  {{- if .Values.memoryHighWatermark.enabled }}\n  ## Memory Threshold\n  ##\n  total_memory_available_override_value = {{ include "rabbitmq.toBytes" .Values.resources.limits.memory }}\n  vm_memory_high_watermark.{{ .Values.memoryHighWatermark.type }} = {{ .Values.memoryHighWatermark.value }}\n  {{- end }}\n\n## Configuration file content: extra configuration\n## Use this instead of `configuration` to add more configuration\n##\nextraConfiguration: |-\n  #default_vhost = {{ .Release.Namespace }}-vhost\n  #disk_free_limit.absolute = 50MB\n  #load_definitions = /app/load_definition.json\n\n## Configuration file content: advanced configuration\n## Use this as additional configuration in classic config format (Erlang term configuration format)\n##\n## If you set LDAP with TLS/SSL enabled and you are using self-signed certificates, uncomment these lines.\n## advancedConfiguration: |-\n##   [{\n##     rabbitmq_auth_backend_ldap,\n##     [{\n##         ssl_options,\n##         [{\n##             verify, verify_none\n##         }, {\n##             fail_if_no_peer_cert,\n##             false\n##         }]\n##     ]}\n##   }].\n##\nadvancedConfiguration: |-\n\n## LDAP configuration\n##\nldap:\n  enabled: false\n  ## List of LDAP servers hostnames\n  ##\n  servers: []\n  ## LDAP servers port\n  ##\n  port: \'389\'\n  ## Pattern used to translate the provided username into a value to be used for the LDAP bind\n  ## ref: https://www.rabbitmq.com/ldap.html#usernames-and-dns\n  ##\n  user_dn_pattern: cn=${username},dc=example,dc=org\n  tls:\n    ## If you enabled TLS/SSL you can set advaced options using the advancedConfiguration parameter.\n    ##\n    enabled: false\n\n## extraVolumes and extraVolumeMounts allows you to mount other volumes\n## Examples:\n## extraVolumeMounts:\n##   - name: extras\n##     mountPath: /usr/share/extras\n##     readOnly: true\n## extraVolumes:\n##   - name: extras\n##     emptyDir: {}\n##\nextraVolumeMounts: []\nextraVolumes: []\n\n## Optionally specify extra secrets to be created by the chart.\n## This can be useful when combined with load_definitions to automatically create the secret containing the definitions to be loaded.\n## Example:\n## extraSecrets:\n##   load-definition:\n##     load_definition.json: |\n##       {\n##         ...\n##       }\n##\n## Set this flag to true if extraSecrets should be created with <release-name> prepended.\n##\nextraSecretsPrependReleaseName: false\nextraSecrets: {}\n\n## Number of RabbitMQ replicas to deploy\n##\nreplicaCount: 1\n\n## Use an alternate scheduler, e.g. "stork".\n## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/\n##\n# schedulerName:\n\n## RabbitMQ should be initialized one by one when building cluster for the first time.\n## Therefore, the default value of podManagementPolicy is \'OrderedReady\'\n## Once the RabbitMQ participates in the cluster, it waits for a response from another\n## RabbitMQ in the same cluster at reboot, except the last RabbitMQ of the same cluster.\n## If the cluster exits gracefully, you do not need to change the podManagementPolicy\n## because the first RabbitMQ of the statefulset always will be last of the cluster.\n## However if the last RabbitMQ of the cluster is not the first RabbitMQ due to a failure,\n## you must change podManagementPolicy to \'Parallel\'.\n## ref : https://www.rabbitmq.com/clustering.html#restarting\n##\npodManagementPolicy: OrderedReady\n\n## Pod labels. Evaluated as a template\n## Ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\n##\npodLabels: {}\n\n## Pod annotations. Evaluated as a template\n## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/\n##\npodAnnotations: {}\n\n## updateStrategy for RabbitMQ statefulset\n## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies\n##\nupdateStrategyType: RollingUpdate\n\n## Statefulset labels. Evaluated as a template\n## Ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\n##\nstatefulsetLabels: {}\n\n## Name of the priority class to be used by RabbitMQ pods, priority class needs to be created beforehand\n## Ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/\n##\npriorityClassName: \'\'\n\n## Pod affinity preset\n## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n## Allowed values: soft, hard\n##\npodAffinityPreset: ""\n\n## Pod anti-affinity preset\n## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n## Allowed values: soft, hard\n##\npodAntiAffinityPreset: soft\n\n## Node affinity preset\n## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity\n## Allowed values: soft, hard\n##\nnodeAffinityPreset:\n  ## Node affinity type\n  ## Allowed values: soft, hard\n  ##\n  type: ""\n  ## Node label key to match\n  ## E.g.\n  ## key: "kubernetes.io/e2e-az-name"\n  ##\n  key: ""\n  ## Node label values to match\n  ## E.g.\n  ## values:\n  ##   - e2e-az1\n  ##   - e2e-az2\n  ##\n  values: []\n\n## Affinity for pod assignment. Evaluated as a template\n## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity\n## Note: podAffinityPreset, podAntiAffinityPreset, and  nodeAffinityPreset will be ignored when it\'s set\n##\naffinity: {}\n\n## Node labels for pod assignment. Evaluated as a template\n## ref: https://kubernetes.io/docs/user-guide/node-selection/\n##\nnodeSelector: {}\n\n## Tolerations for pod assignment. Evaluated as a template\n## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n##\ntolerations: []\n\n## Topology Spread Constraints for pod assignment spread across your cluster among failure-domains. Evaluated as a template\n## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods\n##\ntopologySpreadConstraints: {}\n\n## RabbitMQ pods\' Security Context\n## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod\n##\npodSecurityContext:\n  enabled: true\n  fsGroup: 1001\n  runAsUser: 1001\n\n## RabbitMQ containers\' Security Context\n## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container\n## Example:\n##   containerSecurityContext:\n##     capabilities:\n##       drop: ["NET_RAW"]\n##     readOnlyRootFilesystem: true\n##\ncontainerSecurityContext: {}\n\n## RabbitMQ containers\' resource requests and limits\n## ref: http://kubernetes.io/docs/user-guide/compute-resources/\n##\nresources:\n  # We usually recommend not to specify default resources and to leave this as a conscious\n  # choice for the user. This also increases chances charts run on environments with little\n  # resources, such as Minikube. If you do want to specify resources, uncomment the following\n  # lines, adjust them as necessary, and remove the curly braces after \'resources:\'.\n  limits: {}\n  #   cpu: 1000m\n  #   memory: 2Gi\n  requests: {}\n  #   cpu: 1000m\n  #   memory: 2Gi\n\n## RabbitMQ containers\' liveness and readiness probes.\n## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes\n##\nlivenessProbe:\n  enabled: true\n  initialDelaySeconds: 120\n  timeoutSeconds: 20\n  periodSeconds: 30\n  failureThreshold: 6\n  successThreshold: 1\nreadinessProbe:\n  enabled: true\n  initialDelaySeconds: 10\n  timeoutSeconds: 20\n  periodSeconds: 30\n  failureThreshold: 3\n  successThreshold: 1\n\n## Custom Liveness probe\n##\ncustomLivenessProbe: {}\n\n## Custom Rediness probe\n##\ncustomReadinessProbe: {}\n\n## Custom Startup probe\n## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-startup-probes\n##\ncustomStartupProbe: {}\n\n## Add init containers to the pod\n## Example:\n## initContainers:\n##   - name: your-image-name\n##     image: your-image\n##     imagePullPolicy: Always\n##     ports:\n##       - name: portname\n##         containerPort: 1234\n##\ninitContainers: {}\n\n## Add sidecars to the pod.\n## Example:\n## sidecars:\n##   - name: your-image-name\n##     image: your-image\n##     imagePullPolicy: Always\n##     ports:\n##       - name: portname\n##         containerPort: 1234\n##\nsidecars: {}\n\n## RabbitMQ pods ServiceAccount\n## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n##\nserviceAccount:\n  ## Specifies whether a ServiceAccount should be created\n  ##\n  create: true\n  ## The name of the ServiceAccount to use.\n  ## If not set and create is true, a name is generated using the rabbitmq.fullname template\n  ##\n  # name:\n\n## Role Based Access\n## ref: https://kubernetes.io/docs/admin/authorization/rbac/\n##\nrbac:\n  ## Whether RBAC rules should be created\n  ## binding RabbitMQ ServiceAccount to a role\n  ## that allows RabbitMQ pods querying the K8s API\n  ##\n  create: true\n\npersistence:\n  ## this enables PVC templates that will create one per pod\n  ##\n  enabled: true\n\n  ## rabbitmq data Persistent Volume Storage Class\n  ## If defined, storageClassName: <storageClass>\n  ## If set to "-", storageClassName: "", which disables dynamic provisioning\n  ## If undefined (the default) or set to null, no storageClassName spec is\n  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on\n  ##   GKE, AWS & OpenStack)\n  ##\n  # storageClass: "-"\n  ## selector can be used to match an existing PersistentVolume\n  ## selector:\n  ##   matchLabels:\n  ##     app: my-app\n  ##\n  selector: {}\n  accessMode: ReadWriteOnce\n\n  ## Existing PersistentVolumeClaims\n  ## The value is evaluated as a template\n  ## So, for example, the name can depend on .Release or .Chart\n  # existingClaim: ""\n\n  ## If you change this value, you might have to adjust `rabbitmq.diskFreeLimit` as well.\n  ##\n  size: 8Gi\n\n  volumes:\n  #  - name: volume_name\n  #    emptyDir: {}\n\n## Pod Disruption Budget configuration\n## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/\n##\npdb:\n  create: false\n  ## Min number of pods that must still be available after the eviction\n  ##\n  minAvailable: 1\n  ## Max number of pods that can be unavailable after the eviction\n  ##\n  # maxUnavailable: 1\n\n## Network Policy configuration\n## ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/\n##\nnetworkPolicy:\n  ## Enable creation of NetworkPolicy resources\n  ##\n  enabled: false\n  ## The Policy model to apply. When set to false, only pods with the correct\n  ## client label will have network access to the ports RabbitMQ is listening\n  ## on. When true, RabbitMQ will accept connections from any source\n  ## (with the correct destination port).\n  ##\n  allowExternal: true\n  ## Additional NetworkPolicy Ingress "from" rules to set. Note that all rules are OR-ed.\n  ##\n  # additionalRules:\n  #  - matchLabels:\n  #    - role: frontend\n  #  - matchExpressions:\n  #    - key: role\n  #      operator: In\n  #      values:\n  #        - frontend\n\n## Kubernetes service type\n##\nservice:\n  type: ClusterIP\n  ## Amqp port\n  ## ref: https://github.com/bitnami/bitnami-docker-rabbitmq#environment-variables\n  ##\n  port: 5672\n\n  ## Amqp service port name\n  ##\n  portName: amqp\n\n  ## Amqp Tls port\n  ##\n  tlsPort: 5671\n\n  ## Amqp Tls service port name\n  ##\n  tlsPortName: amqp-ssl\n\n  ## Node port\n  ## ref: https://github.com/bitnami/bitnami-docker-rabbitmq#environment-variables\n  ##\n  # nodePort: 30672\n\n  ## Node port Tls\n  ##\n  # tlsNodePort: 30671\n\n  ## Dist port\n  ## ref: https://github.com/bitnami/bitnami-docker-rabbitmq#environment-variables\n  ##\n  distPort: 25672\n\n  ## Dist service port name\n  ##\n  distPortName: dist\n\n  ## Node port (Manager)\n  ##\n  # distNodePort: 30676\n\n  ## RabbitMQ Manager port\n  ## ref: https://github.com/bitnami/bitnami-docker-rabbitmq#environment-variables\n  ##\n  managerPortEnabled: true\n\n  managerPort: 15672\n\n  ## RabbitMQ Manager service port name\n  ##\n  managerPortName: http-stats\n\n  ## Node port (Manager)\n  ##\n  # managerNodePort: 30673\n\n  ## RabbitMQ Prometheues metrics port\n  ##\n  metricsPort: 9419\n\n  ## RabbitMQ Prometheues metrics service port name\n  ##\n  metricsPortName: metrics\n\n  ## Node port for metrics\n  ##\n  # metricsNodePort: 30674\n\n  ## Node port for EPMD Discovery\n  ##\n  # epmdNodePort: 30675\n\n  ## Service port name for EPMD Discovery\n  ##\n  epmdPortName: epmd\n\n  ## Extra ports to expose\n  ## E.g.:\n  ## extraPorts:\n  ## - name: new_svc_name\n  ##   port: 1234\n  ##   targetPort: 1234\n  ##\n  extraPorts: []\n\n  ## Load Balancer sources\n  ## https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service\n  ##\n  # loadBalancerSourceRanges:\n  # - 10.10.10.0/24\n\n  ## Set the ExternalIPs\n  ##\n  # externalIPs:\n\n  ## Enable client source IP preservation\n  ## ref http://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip\n  ##\n  externalTrafficPolicy: Cluster\n\n  ## Set the LoadBalancerIP\n  ##\n  # loadBalancerIP:\n\n  ## Service labels. Evaluated as a template\n  ##\n  labels: {}\n\n  ## Service annotations. Evaluated as a template\n  ## Example:\n  ## annotations:\n  ##   service.beta.kubernetes.io/aws-load-balancer-internal: 0.0.0.0/0\n  ##\n  annotations: {}\n  ## Headless Service annotations. Evaluated as a template\n  ## Example:\n  ## annotations:\n  ##   external-dns.alpha.kubernetes.io/internal-hostname: rabbitmq.example.com\n  ##\n  annotationsHeadless: {}\n\n## Configure the ingress resource that allows you to access the\n## RabbitMQ installation. Set up the URL\n## ref: http://kubernetes.io/docs/user-guide/ingress/\n##\ningress:\n  ## Set to true to enable ingress record generation\n  ##\n  enabled: false\n\n  ## Path for the default host. You may need to set this to \'/*\' in order to use this\n  ## with ALB ingress controllers.\n  ##\n  path: /\n\n  ## Ingress Path type\n  ##\n  pathType: ImplementationSpecific\n\n  ## Set this to true in order to add the corresponding annotations for cert-manager\n  ##\n  certManager: false\n\n  ## When the ingress is enabled, a host pointing to this will be created\n  ##\n  hostname: rabbitmq.local\n\n  ## Ingress annotations done as key:value pairs\n  ## For a full list of possible ingress annotations, please see\n  ## ref: https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md\n  ##\n  ## If certManager is set to true, annotation kubernetes.io/tls-acme: "true" will automatically be set\n  ##\n  annotations: {}\n\n  ## Enable TLS configuration for the hostname defined at ingress.hostname parameter\n  ## TLS certificates will be retrieved from a TLS secret with name: {{- printf "%s-tls" .Values.ingress.hostname }}\n  ## or a custom one if you use the tls.existingSecret parameter\n  ## You can use the ingress.secrets parameter to create this TLS secret or relay on cert-manager to create it\n  ##\n  tls: false\n  ## existingSecret: name-of-existing-secret\n  ##\n\n  ## The list of additional hostnames to be covered with this ingress record.\n  ## Most likely the hostname above will be enough, but in the event more hosts are needed, this is an array\n  ## extraHosts:\n  ## - name: rabbitmq.local\n  ##   path: /\n  ##\n\n  ## The tls configuration for additional hostnames to be covered with this ingress record.\n  ## see: https://kubernetes.io/docs/concepts/services-networking/ingress/#tls\n  ## extraTls:\n  ## - hosts:\n  ##     - rabbitmq.local\n  ##   secretName: rabbitmq.local-tls\n  ##\n\n  ## If you\'re providing your own certificates, please use this to add the certificates as secrets\n  ## key and certificate should start with -----BEGIN CERTIFICATE----- or\n  ## -----BEGIN RSA PRIVATE KEY-----\n  ##\n  ## name should line up with a tlsSecret set further up\n  ## If you\'re using cert-manager, this is unneeded, as it will create the secret for you if it is not set\n  ##\n  ## It is also possible to create and manage the certificates outside of this helm chart\n  ## Please see README.md for more information\n  ##\n  secrets: []\n  ## - name: rabbitmq.local-tls\n  ##   key:\n  ##   certificate:\n  ##\n\n## Prometheus Metrics\n##\nmetrics:\n  enabled: false\n  plugins: \'rabbitmq_prometheus\'\n  ## Prometheus pod annotations\n  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/\n  ##\n  podAnnotations:\n    prometheus.io/scrape: \'true\'\n    prometheus.io/port: \'{{ .Values.service.metricsPort }}\'\n\n  ## Prometheus Service Monitor\n  ## ref: https://github.com/coreos/prometheus-operator\n  ##\n  serviceMonitor:\n    ## If the operator is installed in your cluster, set to true to create a Service Monitor Entry\n    ##\n    enabled: false\n    ## Specify the namespace in which the serviceMonitor resource will be created\n    ##\n    # namespace: ""\n    ## Specify the interval at which metrics should be scraped\n    ##\n    interval: 30s\n    ## Specify the timeout after which the scrape is ended\n    ##\n    # scrapeTimeout: 30s\n    ## Specify Metric Relabellings to add to the scrape endpoint\n    ##\n    # relabellings:\n    ## Specify honorLabels parameter to add the scrape endpoint\n    ##\n    honorLabels: false\n    ## Specify the release for ServiceMonitor. Sometimes it should be custom for prometheus operator to work\n    ##\n    # release: ""\n    ## Used to pass Labels that are used by the Prometheus installed in your cluster to select Service Monitors to work with\n    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#prometheusspec\n    ##\n    additionalLabels: {}\n\n  ## Custom PrometheusRule to be defined\n  ## The value is evaluated as a template, so, for example, the value can depend on .Release or .Chart\n  ## ref: https://github.com/coreos/prometheus-operator#customresourcedefinitions\n  ##\n  prometheusRule:\n    enabled: false\n    additionalLabels: {}\n    namespace: \'\'\n    ## List of rules, used as template by Helm.\n    ## These are just examples rules inspired from https://awesome-prometheus-alerts.grep.to/rules.html\n    # rules:\n    #   - alert: RabbitmqDown\n    #     expr: rabbitmq_up{service="{{ template "rabbitmq.fullname" . }}"} == 0\n    #     for: 5m\n    #     labels:\n    #       severity: error\n    #     annotations:\n    #       summary: Rabbitmq down (instance {{ "{{ $labels.instance }}" }})\n    #       description: RabbitMQ node down\n    #   - alert: ClusterDown\n    #     expr: |\n    #       sum(rabbitmq_running{service="{{ template "rabbitmq.fullname" . }}"})\n    #       < {{ .Values.replicaCount }}\n    #     for: 5m\n    #     labels:\n    #       severity: error\n    #     annotations:\n    #       summary: Cluster down (instance {{ "{{ $labels.instance }}" }})\n    #       description: |\n    #           Less than {{ .Values.replicaCount }} nodes running in RabbitMQ cluster\n    #           VALUE = {{ "{{ $value }}" }}\n    #   - alert: ClusterPartition\n    #     expr: rabbitmq_partitions{service="{{ template "rabbitmq.fullname" . }}"} > 0\n    #     for: 5m\n    #     labels:\n    #       severity: error\n    #     annotations:\n    #       summary: Cluster partition (instance {{ "{{ $labels.instance }}" }})\n    #       description: |\n    #           Cluster partition\n    #           VALUE = {{ "{{ $value }}" }}\n    #   - alert: OutOfMemory\n    #     expr: |\n    #       rabbitmq_node_mem_used{service="{{ template "rabbitmq.fullname" . }}"}\n    #       / rabbitmq_node_mem_limit{service="{{ template "rabbitmq.fullname" . }}"}\n    #       * 100 > 90\n    #     for: 5m\n    #     labels:\n    #       severity: warning\n    #     annotations:\n    #       summary: Out of memory (instance {{ "{{ $labels.instance }}" }})\n    #       description: |\n    #           Memory available for RabbmitMQ is low (< 10%)\\n  VALUE = {{ "{{ $value }}" }}\n    #           LABELS: {{ "{{ $labels }}" }}\n    #   - alert: TooManyConnections\n    #     expr: rabbitmq_connectionsTotal{service="{{ template "rabbitmq.fullname" . }}"} > 1000\n    #     for: 5m\n    #     labels:\n    #       severity: warning\n    #     annotations:\n    #       summary: Too many connections (instance {{ "{{ $labels.instance }}" }})\n    #       description: |\n    #           RabbitMQ instance has too many connections (> 1000)\n    #           VALUE = {{ "{{ $value }}" }}\\n  LABELS: {{ "{{ $labels }}" }}\n    rules: []\n\n## Init Container parameters\n## Change the owner and group of the persistent volume(s) mountpoint(s) to \'runAsUser:fsGroup\' on each component\n## values from the securityContext section of the component\n##\nvolumePermissions:\n  enabled: false\n  image:\n    registry: docker.io\n    repository: bitnami/bitnami-shell\n    tag: "10"\n    ## Specify a imagePullPolicy\n    ## Defaults to \'Always\' if image tag is \'latest\', else set to \'IfNotPresent\'\n    ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images\n    ##\n    pullPolicy: Always\n    ## Optionally specify an array of imagePullSecrets (secrets must be manually created in the namespace)\n    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n    ## Example:\n    ## pullSecrets:\n    ##   - myRegistryKeySecretName\n    ##\n    pullSecrets: []\n  ## Init Container resource requests and limits\n  ## ref: http://kubernetes.io/docs/user-guide/compute-resources/\n  ##\n  resources:\n    # We usually recommend not to specify default resources and to leave this as a conscious\n    # choice for the user. This also increases chances charts run on environments with little\n    # resources, such as Minikube. If you do want to specify resources, uncomment the following\n    # lines, adjust them as necessary, and remove the curly braces after \'resources:\'.\n    limits: {}\n    #   cpu: 100m\n    #   memory: 128Mi\n    requests: {}\n    #   cpu: 100m\n    #   memory: 128Mi\n')))}d.isMDXComponent=!0}}]);